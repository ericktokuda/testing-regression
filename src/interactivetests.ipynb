{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import Ridge\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "rawdata0 = pd.read_csv('20170709-pedestrians.csv')\n",
    "#rawdata = pandas.read_csv('20170709-pedestrians_sample.csv')\n",
    "rawdata0.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawdata0.describe()\n",
    "\n",
    "#rawdata0.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plt.hist(rawdata0['dx'])\n",
    "# rawdata0.dtypes\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "axs[0].hist(rawdata0['dx'], bins=20)\n",
    "axs[1].hist(rawdata0['dy'], bins=20)\n",
    "plt.show()\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "axs[0].hist(rawdata0['ddays'], bins=20)\n",
    "counts, bins, patches = axs[1].hist(rawdata0['time'], bins=20)\n",
    "#axs[1].set_xticks(bins)\n",
    "plt.show()\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "h = axs[0].hist(rawdata0['dow'], bins=20)\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "counts, bins, patches = plt.hist(rawdata0['time'], bins=20)\n",
    "k = plt.xticks(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filering tails of the histogram in the hour plot\n",
    "rawdata = rawdata0[(rawdata0['time'] > 7) & (rawdata0['time'] <= 19) ]\n",
    "rawdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "ndata = rawdata.copy()\n",
    "np_scaled = min_max_scaler.fit_transform(ndata[['dx', 'dy', 'ddays', 'time', 'dow']])\n",
    "ndata[['dx', 'dy', 'ddays', 'time', 'dow']] = np_scaled\n",
    "ndata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "mydata = rawdata\n",
    "mod = smf.ols('people ~ dx + dy + ddays + time', data=mydata)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "mydata = rawdata[rawdata.dow == 1]\n",
    "mod = smf.ols('people ~ dx + dy + ddays + time', data=mydata)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Regression\n",
    "Got the code below from http://planspace.org/20150423-forward_selection_with_statsmodels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def forward_selected(data, response):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "           evaluated by adjusted R-squared\n",
    "    \"\"\"\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(response,\n",
    "                                           ' + '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    formula = \"{} ~ {} + 1\".format(response,\n",
    "                                   ' + '.join(selected))\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "model = forward_selected(mydata[['dx', 'dy', 'ddays', 'time', 'people']], 'people')\n",
    "print(model.model.formula)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "clf = Ridge(alpha=0.5)\n",
    "res = clf.fit(mydata[['dx', 'dy', 'ddays', 'time']], mydata[['people']])\n",
    "res.score(mydata[['dx', 'dy', 'ddays', 'time']], mydata[['people']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy off-the-shelf algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy offers interesting interpolation approaches:\n",
    "https://docs.scipy.org/doc/scipy/reference/interpolate.html\n",
    "\n",
    "They provide a set of tools for scattered data, but among them, just two drawed my attention:\n",
    "* nearest neighbour\n",
    "* rbf\n",
    "\n",
    "Matlab also provides some methods for scattered points interpolations, such as `griddatan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "points = mydata[['dx', 'dy', 'ddays', 'time']].as_matrix()\n",
    "values = mydata[['people']].as_matrix()\n",
    "mins = pd.DataFrame.min(mydata[['dx', 'dy', 'ddays', 'time']])\n",
    "maxs = pd.DataFrame.max(mydata[['dx', 'dy', 'ddays', 'time']])\n",
    "npartitions = 20\n",
    "griddx, griddy,gridddays, gridtime = np.mgrid[mins['dx']:maxs['dx']:npartitions,\n",
    "         mins['dy']:maxs['dy']:npartitions,\n",
    "         mins['ddays']:maxs['ddays']:npartitions,\n",
    "         mins['time']:maxs['time']:npartitions,\n",
    "        ]\n",
    "\n",
    "#Each segment below takes ~30min in a single-threaded run\n",
    "#grid = griddata(points, values, (griddx, griddy, gridddays, gridtime), method='nearest')\n",
    "#np.save('gridnearest.npy', grid)\n",
    "\n",
    "#Linear interpolation returns just NaN values\n",
    "#grid = griddata(points, values, (griddx, griddy, gridddays, gridtime), method='linear')\n",
    "#np.save('gridlinear.npy', grid)\n",
    "\n",
    "#Gives error. Cubic is for 1d or 2d data\n",
    "#grid = griddata(points, values, (griddx, griddy, gridddays, gridtime), method='cubic')\n",
    "\n",
    "#np.save('gridcubic.npy', grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates memory error. Very memory intensive, even using 64GB in the cluster\n",
    "#from scipy.interpolate import Rbf\n",
    "#rbfi = Rbf(points[:,0],points[:,1],points[:,2],points[:,3], values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SVM RBF Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, preprocessing\n",
    "import pandas\n",
    "import time\n",
    "\n",
    "# For a sample of size 100000 it took 608.8498375415802\n",
    "t0 = time.time()\n",
    "samplesz = 10000\n",
    "_data = ndata.sample(n=samplesz)\n",
    "skpoints = _data[['dx', 'dy', 'ddays', 'time']]\n",
    "clf = svm.SVR(kernel='rbf', C=1.0, verbose=True, cache_size=20000)\n",
    "#clf.fit(skpoints, np.array(_data[['people']]).ravel() )\n",
    "#print(clf.predict(skpoints.sample(n=1)[['dx', 'dy', 'ddays', 'time']]))\n",
    "#print('For a sample of size {} it took {}'.format(samplesz, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Partition in train/val and test\n",
    "from numpy import random\n",
    "trainratio = 0.8\n",
    "msk = np.random.rand(len(ndata)) < trainratio\n",
    "traindata = ndata[msk]\n",
    "testdata  = ndata[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I compute the suppport\n",
    "scaledddx = 1000.0/(np.max(rawdata['dx']) - np.min(rawdata['dx']))\n",
    "scaledddy = 1000.0/(np.max(rawdata['dy']) - np.min(rawdata['dy']))\n",
    "scaledddays = 5.0/326\n",
    "scaleddtime = 4.0/(np.max(rawdata['time']) - np.min(rawdata['time']))\n",
    "print(scaledddx, scaledddy, scaledddays, scaleddtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 0.6\n",
    "filtereddata = ndata.copy()\n",
    "filtereddata = filtereddata[(filtereddata['dx'] < p + scaledddx) & (filtereddata['dx'] > p - scaledddx)]\n",
    "print(filtereddata['dx'].count())\n",
    "filtereddata = filtereddata[(filtereddata['dy'] < p + scaledddy) & (filtereddata['dy'] > p - scaledddy)]\n",
    "print(filtereddata['dx'].count())\n",
    "filtereddata = filtereddata[(filtereddata['ddays'] < p + scaledddays) & (filtereddata['ddays'] > p - scaledddays)]\n",
    "print(filtereddata['dx'].count())\n",
    "filtereddata = filtereddata[(filtereddata['time'] < p + scaleddtime) & (filtereddata['time'] > p - scaleddtime)]\n",
    "print(filtereddata['dx'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mins = (np.min(ndata)[['dx', 'dy', 'ddays', 'time']]).as_matrix()\n",
    "maxs = np.max(ndata)[['dx', 'dy', 'ddays', 'time']].as_matrix()\n",
    "steps = [scaledddx, scaledddy, scaledddays, scaleddtime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "mygrid = list(product(*[np.arange(i, j, k)[:-1] for i,j,k in zip(mins, maxs, steps)]))\n",
    "acc = 0\n",
    "for p in mygrid:\n",
    "#     print('###############################################')\n",
    "#     print(p)\n",
    "    filtereddata = traindata.copy()\n",
    "    filtereddata = filtereddata[(filtereddata['dx'] < p[0] + scaledddx) & (filtereddata['dx'] > p[0] - scaledddx)]\n",
    "#     print(len(filtereddata.index))\n",
    "    filtereddata = filtereddata[(filtereddata['dy'] < p[1] + scaledddy) & (filtereddata['dy'] > p[1] - scaledddy)]\n",
    "#     print(len(filtereddata.index))\n",
    "    filtereddata = filtereddata[(filtereddata['ddays'] < p[2] + scaledddays) & (filtereddata['ddays'] > p[2] - scaledddays)]\n",
    "#     print(len(filtereddata.index))\n",
    "    filtereddata = filtereddata[(filtereddata['time'] < p[3] + scaleddtime) & (filtereddata['time'] > p[3] - scaleddtime)]\n",
    "    sz = len(filtereddata.index)\n",
    "    \n",
    "    if sz > 3:\n",
    "        print(p)\n",
    "        print(acc, sz)\n",
    "        acc += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(traindata.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
